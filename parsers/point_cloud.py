import math
import os
import random
from enum import Enum
from typing import Optional

import geom
import laspy
import numpy as np
import scipy
import startinpy

import raster
import utils

random.seed(20060929776431413 % 32)


class DownsamplingMethod(Enum):
    """
    A method which can be used to decrease the sample rate of a point cloud.

    See :meth:`PointCloud.downsample<point_cloud.PointCloud.downsample>` for more information.
    """
    NTH_POINT = 0,
    """
    The sample rate of the point cloud is decreased by keeping the first point and then every ``n``-th one after that.
    """
    RANDOM = 1
    """
    The sample rate of the point cloud is decreased by keeping a certain number of randomly selected points.
    """


class DTMGenerationMethod(Enum):
    """
    A method which can be used to generate the digital terrain model (DTM) of a point cloud.

    See :meth:`PointCloud.dtm<point_cloud.PointCloud.dtm>` for more information.
    """
    CLASSIFICATION = 0,
    """
    The DTM is generated by first classifying its ground points, and then using the resulting information to interpolate
    the terrain elevation at the locations of the center points of corresponding raster cells.
    """
    CLOTH = 1
    """
    The DTM is generated by first classifying its ground points, and then using the intermediate representation of the
    terrain to represent it.
    """


class PointCloud:
    """A point cloud contained inside a LAS/Z file."""

    def __init__(self, filename: str) -> None:
        with laspy.open(filename) as f:
            self.data = f.read()

    def crop(self, bbox: geom.BoundingBox) -> None:
        self.data.points = self.data.points[bbox.contains(self.data.xyz)]

        # Ensure that the underlying point record remains contiguous in memory.
        if not self.data.points.array.flags['C_CONTIGUOUS']:
            self.data.points.array = np.ascontiguousarray(self.data.points.array)

    def csf(self,
            filename: Optional[str] = None,
            resolution: Optional[float] = 2,
            timestep: Optional[float] = 0.2,
            stiffness: Optional[float] = 1,
            eps_sim: Optional[float] = 1e-3,
            eps_cls: Optional[float] = 0.5,
            initial_elevation: Optional[float] = None) -> None:
        """

        Args:
            filename:
                A string representing the relative or absolute system path to the output file.
            resolution:
                A non-negative number representing the size of each cell of the particle grid which is used to model the
                cloth in the units used by the coordinate reference system of the point cloud.
            timestep:
                A non-negative number representing the time step of the simulation in seconds.
            stiffness:
                A non-negative number between 0 and 1 representing the rigidity of the cloth.
            eps_sim:
                A non-negative number representing the maximum vertical displacement of any particle of the cloth
                between two consecutive time steps for the simulation to be terminated.
            eps_cls:
                A non-negative number representing the maximum vertical distance between any point of the point cloud
                and the cloth in equilibrium for it to be classified as a terrain feature.
            initial_elevation:
                The initial elevation of the cloth in the units used by the coordinate reference system of the point
                cloud.
        """
        # Invert the point cloud.
        self.data.z *= -1

        if initial_elevation is None:
            # Initialize the cloth at an arbitrary height above the point cloud.
            initial_elevation = np.max(self.data.z) + (
                        np.max(self.data.z) - np.min(self.data.z))

        # Model the cloth as a 3D array representing a uniformly distributed particle grid.
        x = np.arange(np.min(self.data.x), np.max(self.data.x) + resolution, resolution)
        y = np.arange(np.min(self.data.y), np.max(self.data.y) + resolution, resolution)
        # Initialize the cloth at a safe height above the point cloud.
        z = np.full([x.shape[0], y.shape[0]],
                    # Cast the initial elevation of the cloth into a floating-point value in order to allow for a more
                    # precise simulation.
                    float(initial_elevation))

        # This is the cloth.
        c = np.array([*np.meshgrid(x, y), z, # All particles are initially movable.
                      np.ones(z.shape)])

        # Compute the minimum elevation of each particle.
        # Insert the particles into a k-d tree.
        idx = scipy.spatial.KDTree(np.column_stack([self.data.x, self.data.y]))
        # Compute the elevation of the nearest neighbor of each particle in the point cloud.
        qrs = np.array([c[0, :, :].reshape(-1),
                        c[1, :, :].reshape(-1)]).transpose().tolist()
        # Populate the corresponding array.
        # This suppression is so that PyCharm does not complain about objects of unexpected types being used.
        # noinspection PyTypeChecker
        min_z = np.reshape(self.data.z[idx.query(qrs, workers=-1)[1]], z.shape)

        # Compute the elevation of each particle at the previous time step.
        # TODO - Explain the magic numbers.
        prv_z = z + 0.5 * 9.80655 * timestep ** 2

        # Compute the change in the elevation of each particle between the current and the previous time step.
        dz = c[2, :, :] - prv_z
        max_dz = np.abs(np.amax(dz))

        # def internal(ij0, ij1):
        #     try:
        #         d = (c[2,ij0[0], ij0[1]] - c[2,ij1[0], ij1[1]])
        #
        #         if c[3,ij1[0], ij1[1]]:
        #             c[2,ij0[0], ij0[1]] += 0.5 * stiffness * d
        #             c[2,ij1[0], ij1[1]] -= 0.5 * stiffness * d
        #         else:
        #             c[2,ij0[0], ij0[1]] += stiffness * d
        #     except IndexError:
        #         pass

        print(f"{max_dz=}")
        iterations = 0

        while max_dz > eps_sim:
            print(f"{iterations=}")
            for i in range(c.shape[1]):
                for j in range(c.shape[2]):
                    # Consider the external forces (i.e., gravity) acting on the cloth.
                    if c[3, i, j]:
                        prv_z[i, j], c[2, i, j] = c[2, i, j], 2 * c[2, i, j] - prv_z[
                            i, j]

            # Consider the internal forces acting on the cloth. These can be assumed to be due to coil springs
            # connecting adjacent particles and creating tension within the cloth.
            #
            # For each particle (i, j), there are four adjacent particles, namely:
            #     (i - 1, j)
            #     (i + 1, j)
            #     (i, j - 1)
            #     (i, j + 1)
            # This allows the loop over the springs to be converted to one over the particles.

            for i in range(c.shape[1]):
                for j in range(c.shape[2]):
                    if c[3, i, j]:

                        if (i - 1) < 0:
                            top = c.shape[1]
                        else:
                            top = i - 1
                        if (j - 1) < 0:
                            left = c.shape[2]
                        else:
                            left = j - 1

                        try:
                            dtop = (c[2, top, j] - c[2, i, j]) * 0.5
                            if c[3, top, j]:
                                c[2, i, j] += stiffness * dtop
                                c[2, top, j] -= stiffness * dtop
                            elif not math.isclose(c[2, i, j], c[2, top, j]):
                                c[2, i, j] += stiffness * dtop * 2
                        except IndexError:
                            pass

                        try:
                            dbottom = (c[2, i + 1, j] - c[2, i, j]) * 0.5
                            if c[3, i + 1, j]:
                                c[2, i, j] += stiffness * dbottom
                                c[2, i + 1, j] -= stiffness * dbottom
                            elif not math.isclose(c[2, i, j], c[2, i + 1, j]):
                                c[2, i, j] += stiffness * dbottom * 2
                        except IndexError:
                            pass

                        try:
                            dleft = (c[2, i, left] - c[2, i, j]) * 0.5
                            if c[3, i, left]:
                                c[2, i, j] += stiffness * dleft
                                c[2, i, left] -= stiffness * dleft
                            elif not math.isclose(c[2, i, j], c[2, i, left]):
                                c[2, i, j] += stiffness * dleft * 2
                        except IndexError:
                            pass

                        try:
                            dright = (c[2, i, j + 1] - c[2, i, j]) * 0.5
                            if c[3, i, j + 1]:
                                c[2, i, j] += stiffness * dright
                                c[2, i, j + 1] -= stiffness * dright
                            elif not math.isclose(c[2, i, j], c[2, i, j + 1]):
                                c[2, i, j] += stiffness * dright * 2
                        except IndexError:
                            pass

            # Check for possible intersections of the cloth with the ground.
            for i in range(c.shape[1]):
                for j in range(c.shape[2]):
                    if c[2, i, j] < min_z[i, j]:
                        c[2, i, j] = min_z[i, j]
                        c[3, i, j] = 0

            # Compute the change in the elevation of each particle between this and the previous time step.
            dz = c[2, :, :] - prv_z
            if max_dz > np.abs(np.max(dz)):
                max_dz = np.abs(np.max(dz))

            print(f"{max_dz=}")
            iterations += 1

        # Invert the point cloud.
        self.data.z *= -1

        if filename is not None:
            # Create the required directory to store the output file.
            utils.mkdirs(filename)

            # Serialize the cloth.
            with open(filename, "wb") as f:
                # This suppression is so that PyCharm does not complain about objects of unexpected types being used.
                # noinspection PyTypeChecker
                np.save(f, np.array([c[0, :, :], c[1, :, :], -c[2, :, :]]))

        # Classify the ground and non-ground points.

        # Compute the Delaunay triangulation (DT) of the cloth.
        dt = startinpy.DT()
        # This suppression is so that PyCharm does not complain about unexpected arguments being passed to
        # startinpy.DT.insert.
        # noinspection PyArgumentList
        dt.insert(np.column_stack([c[0, :, :].ravel(), c[1, :, :].ravel(),
                                   -c[2, :, :].ravel()]), insertionstrategy="BBox")

        for i, pt in enumerate(self.data.xyz):
            # Compute the elevation of the nearest cloth particle to each point.

            # This nearest neighbor qquery will never raise an Exception because the cloth completely covers the
            # point cloud (i.e., its convex hull completely inscribes it).
            nn = dt.get_point(dt.closest_point(pt[0], pt[1]))
            if np.abs(pt[2] - nn[2]) <= eps_cls:
                self.data.classification[i] = 2
            else:
                self.data.classification[i] = 1

    def downsample(self,
                   percentage: float,
                   method: Optional[
                       DownsamplingMethod] = DownsamplingMethod.RANDOM) -> None:
        """
        Decreases the sample rate of the point cloud by a given percentage.

        Args:
            percentage:
                A non-negative number between 0 and 1 representing the rate at which the point cloud should be
                resampled.
            method:
                The downsampling method that should be used. See
                :class:`DownsamplingMethod<point_cloud.DownsamplingMethod>` for more information.
        """
        if method == DownsamplingMethod.NTH_POINT:
            num_pts = round(percentage ** -1)
            self.data.points = self.data.points[::num_pts]
        else:
            num_pts = round(percentage * len(self.data.xyz))
            self.data.points = self.data.points[
                random.sample(range(len(self.data.xyz)), num_pts)]

        # Ensure that the underlying point record remains contiguous in memory.
        if not self.data.points.array.flags['C_CONTIGUOUS']:
            self.data.points.array = np.ascontiguousarray(self.data.points.array)

    # This suppression is so that PyCharm does not complain about Iterable not declaring a concrete implementation of
    # __getitem__.
    # noinspection PyUnresolvedReferences
    def dtm(self,
            filename: str,
            resolution: float,
            method: Optional[
                DTMGenerationMethod] = DTMGenerationMethod.CLASSIFICATION) -> None:
        """
        Creates the digital terrain model (DTM) which corresponds to the current state of the point cloud and saves
        it to a file.

        Args:
            filename:
                A string representing the relative or absolute system path to the output file.
            resolution:
                A non-negative number representing the size of each cell of the raster in the units used by the
                coordinate reference system of the point cloud.
            method:
                The generation method that should be used. See
                :class:`DTMGenerationMethod<point_cloud.DTMGenerationMethod>` for more information.
        """
        if method == DTMGenerationMethod.CLASSIFICATION:
            self.csf()

            # Get the ground points.
            pts = self.data.xyz[self.data.classification == 2]

            # Compute the Delaunay triangulation (DT) of the points.
            dt = startinpy.DT()
            dt.insert(pts)
        else:
            # TODO - Document this block.
            c_fname = os.path.splitext(filename)[0] + ".npy"
            self.csf(c_fname)

            # Read the cloth into memory.
            with open(c_fname, "rb") as f:
                # This suppression is so that PyCharm does not complain about objects of unexpected types being used.
                # noinspection PyTypeChecker
                c = np.load(f)

            # Compute the Delaunay triangulation (DT) of the cloth.
            dt = startinpy.DT()
            # This suppression is so that PyCharm does not complain about unexpected arguments being passed to
            # startinpy.DT.insert.
            # noinspection PyArgumentList
            dt.insert(np.column_stack([c[0, :, :].ravel(), c[1, :, :].ravel(),
                                       c[2, :, :].ravel()]), insertionstrategy="BBox")

        # Instantiate the raster.
        # FIXME - The bounding box of the DTM may not be exactly correct if DTMGenerationMethod.CLASSIFICATION is used.
        #         This is because the ground points may not be completely spread out in the point cloud.
        dtm = raster.Raster(resolution, dt.get_bbox())

        # Compute the value of each cell of the raster.
        i = 0
        for row in range((dtm.len_y - 1), -1, -1):
            j = 0
            # Compute the location of the center of each cell along the Y-axis.
            y = dtm.bbox[1] + (row * resolution) + (0.5 * resolution)
            for col in range(dtm.len_x):
                # Compute the location of the center of each cell along the X-axis.
                x = dtm.bbox[0] + (col * resolution) + (0.5 * resolution)
                # This suppression is so that PyCharm does not complain about broad exception clauses being
                # used.
                # noinspection PyBroadException
                try:
                    dtm[i, j] = dt.interpolate_laplace(x, y)
                except Exception:
                    pass
                j += 1
            i += 1

        dtm.save(filename)

    def save(self, filename: str) -> None:
        # Create the required directory to store the output file.
        utils.mkdirs(filename)

        with laspy.open(filename, "w", header=self.data.header) as f:
            f.write_points(self.data.points)
