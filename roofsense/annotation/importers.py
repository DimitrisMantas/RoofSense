import collections
import functools
import glob
import itertools
import json
import math
import os
import os.path
import warnings
from abc import ABC
from collections.abc import Sequence
from typing import Any, Literal

import numpy as np
import pandas as pd
import rasterio.errors
import rasterio.mask

from roofsense import config
from roofsense.utils.geom import read_surfaces


class DatasetImporter(ABC):
    """Abstract dataset importer."""

    pass


class RoboflowDatasetImporter:
    """Roboflow dataset importer."""

    # TODO: Read this from the dataset.
    class_names = {
        0: "Background",
        1: "Asphalt Shingles",
        2: "Bituminous Coating / Membranes",
        3: "Ceramic Tiles",
        4: "Concrete",
        5: "Gravel",
        6: "Invalid",
        7: "Light-permitting Opening",
        8: "Metal",
        9: "Non-bituminous Coating / Membranes",
        10: "Other",
        11: "Solar Panel Installation",
        12: "Superstructure",
        13: "Vegetation",
    }

    class_colors = {
        0: [0, 0, 0, 255],
        1: [0, 0, 0, 255],
        2: [102, 102, 102, 255],
        3: [170, 109, 58, 255],
        4: [0, 0, 0, 255],
        5: [160, 156, 148, 255],
        6: [0, 0, 0, 255],
        7: [163, 193, 232, 255],
        8: [179, 179, 197, 255],
        9: [225, 225, 225, 255],
        10: [0, 0, 0, 255],
        11: [41, 54, 83, 255],
        12: [0, 0, 0, 255],
        13: [122, 144, 94, 255],
    }

    src_mask_glob = "*.png"
    """The glob pattern used to identify segmentation masks generated by the 
    annotation provider."""

    dst_image_dirname = "imgs"
    """The name of the subdirectory containing the imported images."""

    dst_mask_dirname = "msks"
    """The name of the subdirectory containing the imported masks."""

    split_names = ["training", "validation", "test"]

    def __init__(self, src_dirpath: str) -> None:
        """Configure the importer.

        Args:
            src_dirpath:
                The path to the directory containing the dataset generated by the
                annotation provider.
        """
        # Build the input mask paths.
        # NOTE: Since images and masks have the same name, it is sufficient to keep
        # track of only one collection.
        self.src_maskpaths = glob.glob(os.path.join(src_dirpath, self.src_mask_glob))

        # Dynamic Attributes
        # NOTE: This attribute will be populated when importing the masks.
        self.class_counts: pd.DataFrame | None = None
        # NOTE: This attribute will be populated when splitting the dataset.
        for name in self.split_names:
            self.__dict__[f"{name}_subset"]: list[str] | None = None
        # NOTE: This attribute will be populated when estimating the class weights.
        self.class_weights: np.ndarray[tuple[Any], np.dtype[np.float64]] | None = None

    def __len__(self) -> int:
        return len(self.src_maskpaths)

    @functools.cached_property
    def invalid_index(self) -> int:
        """Fetch the ID of the invalid class in the segmentation masks generated by
        the annotation provider.

        Returns:
            The class ID.
        """
        for cls_index, cls_name in self.class_names.items():
            if "invalid" in cls_name.lower():
                return cls_index

    @staticmethod
    def get_original_filename(filepath: str) -> str:
        """Remove all metadata appended to an image or mask filename by the
        annotation provider.

        Args:
            filepath:
                The path to the file.

        Returns:
            The original filename.
        """
        return os.path.basename(filepath[: filepath.index("_png")]) + ".tif"

    @staticmethod
    def get_tile_id(filepath: str) -> str:
        """Resolve the ID of the 3DBAG tile corresponding to an image or mask
        generated by the annotation provider.

        Args:
            filepath:
                The path to the file.

        Returns:
            The tile ID.
        """
        return os.path.basename(filepath[: filepath.index("_")])

    def import_(
        self,
        dst_dirpath: str,
        lengths: Sequence[float] | None = (0.7, 0.15, 0.15),
        seed: int = 0,
        method: Literal["balanced", "tf-idf"] | None = "tf-idf",
    ) -> None:
        # Import the masks.
        # TODO: Import the masks only when required.
        self._import_masks(os.path.join(dst_dirpath, self.dst_mask_dirname))

        # Split the dataset.
        if lengths is not None:
            self._split(lengths, seed)

        # Estimate the class weights.
        if method is not None:
            if lengths is None:
                msg = (
                    "The dataset must first be split into a training, test and "
                    "validation subset in order for class weights to be correctly "
                    "estimated."
                )
                raise ValueError(msg)
            self._weigh(method)

        self._finalize(  # Mask Metadata
            os.path.join(dst_dirpath, "counts.json"),
            os.path.join(dst_dirpath, "names.json"),
            os.path.join(dst_dirpath, "colors.json"),
            # Splits
            os.path.join(dst_dirpath, "splits.json"),
            # Class Weights
            os.path.join(dst_dirpath, "weights.bin"),
        )

    def _import_masks(self, dst_dirpath: str) -> None:
        # Build the output mask paths.
        dst_paths = [
            os.path.join(dst_dirpath, self.get_original_filename(path))
            for path in self.src_maskpaths
        ]

        # Group the masks by tile.
        # {
        #   tile_id: {
        #              "src": [filename],
        #              "dst": [filename],
        # }
        tile_groups: dict[str, dict[str, list[str]]] = collections.defaultdict(
            functools.partial(collections.defaultdict, list)
        )
        for src_path, dst_path in zip(self.src_maskpaths, dst_paths):
            tile_id = self.get_tile_id(src_path)
            tile_groups[tile_id]["src"].append(src_path)
            tile_groups[tile_id]["dst"].append(dst_path)

        # Preprocess the masks.
        # {
        #   image_name: {
        #                 class_index: pixel_count,
        # }
        class_counts_data: dict[str, dict[int, int]] = collections.defaultdict(dict)
        for tile_id, paths in tile_groups.items():
            surfs = read_surfaces(tile_id).dissolve()
            src_path: str
            dst_path: str
            for src_path, dst_path in zip(paths["src"], paths["dst"]):
                # Fetch the corresponding image profile.
                imagename = dst_path.replace(
                    self.dst_mask_dirname, self.dst_image_dirname
                )
                image_src: rasterio.io.DatasetReader
                with rasterio.open(imagename) as image_src:
                    profile: rasterio.profiles.Profile = image_src.profile
                    profile.update(dtype=np.uint8, nodata=0, count=1)
                # Georeference the masks.
                with warnings.catch_warnings(
                    action="ignore", category=rasterio.errors.NotGeoreferencedWarning
                ):
                    src: rasterio.io.DatasetReader
                    dst: rasterio.io.DatasetReader
                    with (
                        rasterio.open(src_path) as src,
                        rasterio.open(dst_path, mode="w+", **profile) as dst,
                    ):
                        dst.write(src.read())
                        # Remask the background.
                        data, _ = rasterio.mask.mask(dst, shapes=surfs.geometry)
                        # Replace the invalid class.
                        data[data == self.invalid_index] = 0
                        dst.write(data)

                # Update the class counts.
                for class_index, class_count in zip(
                    *np.unique(data[data != 0], return_counts=True)
                ):
                    class_counts_data[self.get_original_filename(src_path)][
                        class_index
                    ] = class_count

        # Format the class counts.
        class_counts = pd.DataFrame.from_dict(class_counts_data, orient="index")
        # Map present class indices to a continuous range.
        class_counts = class_counts.sort_index(axis=1)
        class_indices = {
            old: new for new, old in enumerate(class_counts.columns.values, start=1)
        }
        class_counts = class_counts.rename(columns=class_indices)
        class_counts = class_counts.fillna(0)

        # Postprocess the masks.
        for paths in tile_groups.values():
            for path in paths["dst"]:
                with rasterio.open(path, mode="r+") as src:
                    data = src.read()
                    for old, new in class_indices.items():
                        data[data == old] = new
                    src.write(data)

        # Update the relevant class attributes.
        self.class_counts = class_counts
        self.class_names = {0: self.class_names[0]} | {
            new: self.class_names[old] for old, new in class_indices.items()
        }
        self.class_colors = {0: self.class_colors[0]} | {
            new: self.class_colors[old] for old, new in class_indices.items()
        }

    # TODO: Clean up this method.
    def _split(self, lengths: Sequence[float], seed: int) -> None:
        """Split the dataset into a training, validation and test subset.

        Args:
            lengths:
                The absolute or fractional lengths of each subset.
            seed:
                The seed used to perform the split.

        Returns:
            This importer instance.
        """
        # Initialize the random number generator.
        generator = np.random.default_rng(seed)

        # Fractional Lengths
        if math.isclose(sum(lengths), 1) and sum(lengths) <= 1:
            subset_lengths: list[int] = []
            # Perform the initial split.
            for index, length in enumerate(lengths):
                if length < 0 or length > 1:
                    msg = f"Invalid {self.split_names[index]} split length: {length}."
                    raise RuntimeError(msg)
                abs_length = int(math.floor(len(self) * length))
                subset_lengths.append(abs_length)
            # Assign remaining files to the appropriate subset.
            remainder = len(self) - sum(subset_lengths)
            for index in range(remainder):
                index_to_increment = index % len(subset_lengths)
                subset_lengths[index_to_increment] += 1
            lengths = subset_lengths

            # Validate the split.
            for index, length in enumerate(lengths):
                if length == 0:
                    msg = f"Invalid {self.split_names[index]} split length: {length}."
                    raise RuntimeError(msg)

        # Absolute Lengths
        if sum_len := (sum(lengths)) != len(self):
            msg = (
                f"The specified subset length sum: {sum_len} is not equal to the "
                f"dataset length: {len(self)}."
            )
            raise ValueError(msg)

        # Update the relevant class attributes.
        indices = generator.permutation(sum(lengths))
        for index, (offset, length) in enumerate(
            zip(itertools.accumulate(lengths), lengths)
        ):
            filenames = [
                self.get_original_filename(self.src_maskpaths[file_id])
                for file_id in indices[offset - length : offset]
            ]
            setattr(self, f"{self.split_names[index]}_subset", filenames)

    def _weigh(self, method: Literal["balanced", "tf-idf"]) -> None:
        """Compute class weights over the training subset.

        Returns:
            This importer instance.
        """
        class_counts = self.class_counts.loc[self.training_subset].to_numpy()

        if method == "balanced":
            w = (class_counts.sum(axis=0) / class_counts.sum()) ** -1
        else:
            # Compute the term frequency.
            tf = class_counts / class_counts.sum(axis=1)[:, None]

            # Compute the inverse document frequency.
            idf = class_counts.astype(np.bool_)
            # NOTE: The denominator does not require smoothing because all classes
            # are guaranteed to appear in at least one mask.
            idf = len(idf) / idf.sum(axis=0)
            # NOTE: This scale assumes class equiprobability.
            idf = np.log(idf) / np.log(len(idf))
            # Enforce non-zero weights for classes with are present in every mask.
            idf += 1

            # Reduce the TF-IDF matrix.
            w = (tf.mean(axis=0) * idf) ** -1

        # Normalize the weights.
        w /= w.sum()
        w = np.insert(w, 0, 0)

        # Update the relevant class attributes.
        self.class_weights = w

    def _finalize(
        self,
        class_counts_filepath: str,
        class_names_filepath: str,
        class_colors_filepath: str,
        splits_filepath: str,
        class_weights_filepath: str,
    ) -> None:
        self.class_counts: pd.DataFrame
        self.class_counts.to_json(class_counts_filepath)

        with open(class_names_filepath, mode="w") as f:
            json.dump(self.class_names, f)

        with open(class_colors_filepath, mode="w") as f:
            json.dump(self.class_colors, f)

        splits = {name: getattr(self, f"{name}_subset") for name in self.split_names}
        with open(splits_filepath, mode="w") as f:
            json.dump(splits, f)

        self.class_weights: np.ndarray[tuple[Any], np.dtype[np.float64]]
        self.class_weights.tofile(class_weights_filepath)


if __name__ == "__main__":
    config.config()
    RoboflowDatasetImporter(
        r"C:\Users\Dimit\Downloads\RoofSense 3.v1-batch-1-unchecked-.png-mask-semantic\train"
    ).import_("../dataset/batch_1")
