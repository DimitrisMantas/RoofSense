import functools
import glob
import json
import os
import os.path
import warnings
from abc import ABC, abstractmethod
from collections import OrderedDict, defaultdict
from collections.abc import Collection, Sequence
from enum import UNIQUE, Enum, auto, verify
from operator import itemgetter
from typing import Any, Final

import numpy as np
import pandas as pd
import rasterio.errors
import rasterio.mask
from typing_extensions import override

from roofsense.annotation.exporters import to_clr
from roofsense.bag3d import BAG3DTileStore, LevelOfDetail
from roofsense.utilities.splits import (
    DatasetSplittingMethod,
    random_split,
    stratified_split,
)


@verify(UNIQUE)
class DatasetWeightingMethod(Enum):
    """Methods to weigh training subsets in order to address pertinent class imbalance issues."""

    INV_FREQ = auto()
    """Inverse frequency weighting according to 'sklearn.utils.class_weight.compute_class_weight(class_weight='balanced', ...)'."""
    CUSTOM = auto()
    """TF-IDF--inspired weighting proposed by 'https://resolver.tudelft.nl/uuid:c463e920-61e6-40c5-89e9-25354fadf549'.
    This method mitigates image-level class imbalance in the training set.
    """


class AnnotationImporter(ABC):
    """Base annotation importer."""

    # TODO: Read the class names and colors from the dataset metadata.
    CLASS_NAMES: Final[dict[int, str]] = {
        0: "Background",
        1: "Asphalt Shingle",
        2: "Ceramic Tile",
        3: "Concrete",
        4: "Dark-coloured Membrane",
        5: "Gravel",
        6: "Invalid",
        7: "Light-coloured Membrane",
        8: "Light-permitting Surface",
        9: "Metal",
        10: "Other",
        11: "Solar Panel",
        12: "Stone Shingle",
        13: "Vegetation",
    }
    """The names of the classes supported by the annotation provider.
    Can be joined to 'AnnotationImporter.CLASS_COLORS'.
    """

    CLASS_COLORS: Final[dict[int, Collection[int]]] = {
        0: [0, 0, 0, 255],
        1: [0, 0, 0, 255],
        2: [170, 109, 58, 255],
        3: [0, 0, 0, 255],
        4: [102, 102, 102, 255],
        5: [160, 156, 148, 255],
        6: [0, 0, 0, 255],
        7: [225, 225, 225, 255],
        8: [163, 193, 232, 255],
        9: [179, 179, 179, 255],
        10: [0, 0, 0, 255],
        11: [41, 54, 83, 255],
        12: [0, 0, 0, 255],
        13: [122, 144, 94, 255],
    }
    """The colors of the classes supported by the annotation provider.
    Each color is defined using the RGBA model.
    Can be joined to 'AnnotationImporter.CLASS_NAMES'.
    """

    # TODO: Make this attribute abstract.
    SRC_MASK_GLOB: Final[str] = "*.png"
    """The glob pattern used to identify segmentation masks generated by the annotation provider."""

    #########################################################################################################
    # todo
    dst_image_dirname = "imgs"
    """The name of the dataset subdirectory containing the original images.
    Must already exist.
    """

    dst_mask_dirname = "msks"
    """The name of the dataset subdirectory containing the imported masks."""

    SPLIT_NAMES: Final[list[str]] = ["training", "validation", "test"]

    class_names_filename = "names.json"
    class_colors_filename = "colors.json"
    class_counts_filename = "counts.json"

    splits_filename = "splits.json"
    scales_filename = "scales.bin"
    weights_filename = "weights.bin"
    clr_filename = "colors.clr"

    #########################################################################################################

    def __init__(self, src_dirpath: str, tile_store: BAG3DTileStore) -> None:
        """Initialize the importer.

        Args:
            src_dirpath:
                The path to the directory containing the dataset generated by the annotation provider.
            tile_store:
                The BAG3D tile store used to import the dataset.
        """
        # Build the input mask paths.
        # Since images and masks have the same name, it is sufficient to keep track of only one collection.
        self._src_maskpaths = glob.glob(os.path.join(src_dirpath, self.SRC_MASK_GLOB))
        # This is necessary to replicate the paper results.
        self._src_maskpaths.sort()

        self._tile_store = tile_store

    def __len__(self) -> int:
        return len(self._src_maskpaths)

    # TODO: Generalize this method to return any index.
    # TODO: Check whether an error should be raised if the provided name does not point to an exising class.
    @functools.cached_property
    def invalid_index(self, invalid_name: str = "invalid") -> int | None:
        """Get the index of the invalid class in the segmentation masks generated by the annotation provider.

        Args:
            invalid_name:
                The name of the invalid class. This parameter is case-insensitive.

        Returns:
            The invalid class index.
        """
        for index, name in self.CLASS_NAMES.items():
            if invalid_name.lower() == name.lower():
                return index

    # TODO: Clarify that this method accepts a file path and returns a base name.
    @staticmethod
    @abstractmethod
    def sanitize_filepath(filepath: str) -> str:
        """Remove any metadata added to an image or segmentation mask name by the annotation provider.

        Args:
            filepath:
                The path to the original image or mask.

        Returns:
            The sanitised path.
        """
        ...

    def import_(
        self,
        dst_dirpath: str,
        splitting_method: DatasetSplittingMethod = DatasetSplittingMethod.STRATIFIED,
        weighting_method: DatasetWeightingMethod = DatasetWeightingMethod.CUSTOM,
        lengths: Sequence[float] = (0.70, 0.15, 0.15),
        seed: int = 0,
        replicate_paper: bool = True,
        **kwargs,
    ) -> None:
        os.makedirs(os.path.join(dst_dirpath, self.dst_mask_dirname), exist_ok=True)

        if replicate_paper:
            # TODO: Duplicated code fragment.
            self.CLASS_NAMES = {
                new: self.CLASS_NAMES[old] for old, new in _PAPER_CLASS_MAPPING.items()
            }
            self.CLASS_COLORS = {
                new: self.CLASS_COLORS[old] for old, new in _PAPER_CLASS_MAPPING.items()
            }

        self._import_masks(dst_dirpath, replicate_paper=replicate_paper)
        self._split_dataset(dst_dirpath, splitting_method, lengths, seed, **kwargs)
        self._compute_training_subset_scales(dst_dirpath)
        self._weigh_training_subset(dst_dirpath, weighting_method)

    def _import_masks(self, dst_dirpath: str, replicate_paper: bool) -> None:
        # TODO: Import the masks only when required.
        # Build the output mask paths.
        dst_paths = [
            os.path.join(
                dst_dirpath, self.dst_mask_dirname, self.sanitize_filepath(path)
            )
            for path in self._src_maskpaths
        ]

        # Group the masks by tile.
        # {
        #   tile_id: {
        #              "src": [filepath],
        #              "dst": [filepath],
        # }
        tile_groups: dict[str, dict[str, list[str]]] = defaultdict(
            functools.partial(defaultdict, list)
        )
        for src_path, dst_path in zip(self._src_maskpaths, dst_paths):
            tile_id = self._tile_store.resolve_tile_id(src_path)
            tile_groups[tile_id]["src"].append(src_path)
            tile_groups[tile_id]["dst"].append(dst_path)

        # Preprocess the masks.
        # {
        #   maskpath: {
        #                 class_index: pixel_count,
        # }
        class_counts_dict: dict[str, dict[int, int]] = defaultdict(dict)
        for tile_id, paths in tile_groups.items():
            # Dissolve the surfaces to accelerate background remasking.
            surfs = self._tile_store.read_tile(
                tile_id, lod=LevelOfDetail.LoD22
            ).dissolve()
            src_path: str
            dst_path: str
            for src_path, dst_path in zip(paths["src"], paths["dst"]):
                # Fetch the corresponding image profile.
                imagepath = dst_path.replace(
                    self.dst_mask_dirname, self.dst_image_dirname
                )
                image_src: rasterio.io.DatasetReader
                with rasterio.open(imagepath) as image_src:
                    profile: rasterio.profiles.Profile = image_src.profile
                    profile.update(dtype=np.uint8, nodata=0, count=1)
                # Georeference the masks.
                with warnings.catch_warnings(
                    action="ignore", category=rasterio.errors.NotGeoreferencedWarning
                ):
                    # This object is dynamically typed.
                    src: rasterio.io.DatasetReader | rasterio.io.DatasetWriter
                    dst: rasterio.io.DatasetWriter
                    with (
                        rasterio.open(src_path) as src,
                        rasterio.open(dst_path, mode="w+", **profile) as dst,
                    ):
                        dst.write(src.read())
                        # Remask the background.
                        data, _ = rasterio.mask.mask(dst, shapes=surfs.geometry)
                        # Replace the invalid class.
                        data[data == self.invalid_index] = 0
                        dst.write(data)

                # Update the class counts.
                for class_index, class_count in zip(
                    *np.unique(data[data != 0], return_counts=True)
                ):
                    class_counts_dict[self.sanitize_filepath(src_path)][class_index] = (
                        class_count
                    )

        # Format the class counts.
        class_counts = pd.DataFrame.from_dict(class_counts_dict, orient="index")
        # Ensure the mask order remains sorted.
        class_counts = class_counts.sort_index(axis=0)
        # Map present class indices to a continuous range.
        class_counts = class_counts.sort_index(axis=1)

        if replicate_paper:
            class_mapping = {
                class_index: _PAPER_CLASS_MAPPING[class_index]
                for class_index in class_counts.columns
                if class_index in _PAPER_CLASS_MAPPING
            }
            class_counts = class_counts.rename(columns=class_mapping).reindex(
                columns=sorted(class_mapping.values())
            )

        class_indices = {
            old: new for new, old in enumerate(class_counts.columns.values, start=1)
        }
        class_counts = class_counts.rename(columns=class_indices)
        class_counts = class_counts.fillna(0)

        # Postprocess the masks.
        for paths in tile_groups.values():
            for path in paths["dst"]:
                with rasterio.open(path, mode="r+") as src:
                    data = src.read()
                    for old, new in class_indices.items():
                        data[data == old] = new
                    if replicate_paper:
                        for old, new in _PAPER_CLASS_MAPPING.items():
                            data[data == old] = new
                    src.write(data)

        # Update the relevant class attributes.
        class_counts: pd.DataFrame
        class_counts.to_json(os.path.join(dst_dirpath, self.class_counts_filename))

        # TODO: Duplicated code fragment.
        class_names = {0: self.CLASS_NAMES[0]} | {
            new: self.CLASS_NAMES[old] for old, new in class_indices.items()
        }
        with open(os.path.join(dst_dirpath, self.class_names_filename), mode="w") as f:
            json.dump(class_names, f)

        class_colors = {0: self.CLASS_COLORS[0]} | {
            new: self.CLASS_COLORS[old] for old, new in class_indices.items()
        }
        with open(os.path.join(dst_dirpath, self.class_colors_filename), mode="w") as f:
            json.dump(class_colors, f)

        to_clr(class_names, class_colors, os.path.join(dst_dirpath, self.clr_filename))

    def _split_dataset(
        self,
        dst_dirpath: str,
        method: DatasetSplittingMethod,
        lengths: Sequence[float],
        seed: int,
        **kwargs,
    ) -> None:
        base_kwargs = dict(lengths=lengths, generator=np.random.default_rng(seed))
        class_counts = pd.read_json(
            os.path.join(dst_dirpath, self.class_counts_filename)
        )
        image_names = class_counts.index.tolist()

        if method == DatasetSplittingMethod.RANDOM:
            splits = random_split(len(self), **base_kwargs)
        elif method == DatasetSplittingMethod.STRATIFIED:
            splits = stratified_split(
                class_counts=class_counts.to_numpy(dtype=np.int32),
                **(base_kwargs | kwargs),
            )

        out: defaultdict[str, list[str]] = defaultdict(list)
        for name, split in zip(self.SPLIT_NAMES, splits):
            out[name] = itemgetter(*split)(image_names)

        with open(os.path.join(dst_dirpath, self.splits_filename), mode="w") as f:
            json.dump(out, f)

    def _compute_training_subset_scales(self, dst_dirpath: str) -> None:
        with open(os.path.join(dst_dirpath, self.splits_filename)) as f:
            train_filenames = json.load(f)[self.SPLIT_NAMES[0]]

        mins = np.full(7, fill_value=np.inf)
        maxs = np.full(7, fill_value=-np.inf)
        for name in train_filenames:
            image_path = os.path.join(dst_dirpath, self.dst_image_dirname, name)

            src: rasterio.io.DatasetReader
            with rasterio.open(image_path) as src:
                image = src.read()

            for i, band in enumerate(image):
                mins[i] = min(mins[i], band.min())
                maxs[i] = max(maxs[i], band.max())

        scales = np.asarray([mins, maxs])
        scales.tofile(os.path.join(dst_dirpath, self.scales_filename))

    def _weigh_training_subset(
        self, dst_dirpath: str, method: DatasetWeightingMethod
    ) -> None:
        """Compute class weights over the training subset.

        Returns:
            This importer instance.
        """
        with open(os.path.join(dst_dirpath, self.splits_filename)) as f:
            train_filenames = json.load(f)[self.SPLIT_NAMES[0]]
        class_counts = (
            pd.read_json(os.path.join(dst_dirpath, self.class_counts_filename))
            .loc[train_filenames]
            .to_numpy(dtype=np.int32)
        )

        if method == DatasetWeightingMethod.INV_FREQ:
            w: np.ndarray[tuple[Any,], np.dtype[np.float64]] = (
                class_counts.sum(axis=0) / class_counts.sum()
            ) ** -1
        elif method == DatasetWeightingMethod.CUSTOM:
            # Compute the term frequency.
            tf: np.ndarray[tuple[Any, Any], np.dtype[np.float64]] = class_counts[
                :, None
            ]

            # Compute the inverse document frequency.
            idf: np.ndarray[tuple[Any, Any], np.dtype[np.float64]] = (
                class_counts.astype(np.bool_)
            )
            idf: np.ndarray[tuple[Any, Any], np.dtype[np.float64]] = (len(idf) + 1) / (
                idf.sum(axis=0) + 1
            )
            # This scale assumes class equiprobability.
            idf = np.log(idf) / np.log(len(idf))
            # Enforce non-zero weights for classes with are present in every mask.
            idf += 1

            # Reduce the TF-IDF matrix.
            w = tf.mean(axis=0) ** -1 * idf

        # Normalize the weights.
        w /= w.sum()
        w = np.insert(w, 0, 0)

        # Update the relevant class attributes.
        w.tofile(os.path.join(dst_dirpath, self.weights_filename))


class RoboflowAnnotationImporter(AnnotationImporter):
    @staticmethod
    @override
    def sanitize_filepath(filepath: str) -> str:
        return os.path.basename(filepath[: filepath.index("_png")]) + ".tif"


_PAPER_CLASS_MAPPING = OrderedDict(
    {
        0: 0,
        1: 1,
        2: 3,
        3: 4,
        4: 2,
        5: 5,
        6: 6,
        7: 9,
        8: 7,
        9: 8,
        10: 10,
        11: 11,
        12: 12,
        13: 13,
    }
)
"""The class mapping used in 'https://resolver.tudelft.nl/uuid:c463e920-61e6-40c5-89e9-25354fadf549'.
Can be joined to 'AnnotationImporter.CLASS_NAMES' and 'AnnotationImporter.CLASS_COLORS'.
"""
