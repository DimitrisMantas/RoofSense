{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Identify land use from satellite images using Convolutional Neural Networks - Part 3\n",
        "## Transfer Learning using **ResNet-18**"
      ],
      "metadata": {
        "id": "dJjA3uBaEHXF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook, we will explore the advantages of **transfer learning** for the land use classification task. Transfer learning is a powerful technique where a model developed for a specific task is reused as the starting point for a model on a second task. In our case, we'll utilize the pre-trained `ResNet-18` architecture, a renowned CNN model. For image classification tasks, the pre-trained weights are usually available from different versions of the [ImageNet](https://en.wikipedia.org/wiki/ImageNet). This is a large and diverse dataset containing millions of images across thousands of categories.\n",
        "\n",
        "We will leverage `torchvision.models`, a module within PyTorch's `torchvision` library, to readily obtain pre-trained models and modify them for our purposes. This greatly simplifies the process of applying transfer learning techniques.\n",
        "\n",
        "\n",
        "<img src=\"https://www.researchgate.net/profile/Sajid-Iqbal-13/publication/336642248/figure/fig1/AS:839151377203201@1577080687133/Original-ResNet-18-Architecture_W640.jpg\" width=\"1200\" alt=\"The original ResNet-18 architecture\">\n"
      ],
      "metadata": {
        "id": "A58MFEYEcBU9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The walkthrough part of this notebook will show you some code snippets on how to implement the two main approaches for transfer learning:\n",
        "\n",
        "1. **Fine-tuning all layers**: This approach begins with loading the weights of ResNet18 pre-trained on ImageNet. By fine-tuning all the layers of the architecture, we allow the model to adjust its learned features more precisely to our specific task of land use classification. This method is highly effective when your dataset has some similarities to the original training dataset but still differs in significant ways. Fine-tuning the entire network can lead to better performance as it refines the pre-learned features to be more relevant to the new task. However, this approach requires careful management of the learning rate and other training parameters to ensure that the pre-trained weights are modified appropriately and do not lead to overfitting on the new dataset.\n",
        "\n",
        "2. **Freezing the feature extraction layers and only fine-tuning the classifier**: In this method, we use the architecture of ResNet18 with its pre-trained weights but *freeze* the feature extraction layers. This means that the weights in these layers, which have already learned to extract general features from a broad dataset, remain unchanged. Only the final classification layers of the network are trained to adapt to the specific land use classification task. This approach is particularly useful when the new dataset is quite similar to the one used for pre-training, or when the availability of data for the new task is limited. By keeping the pre-trained features fixed, the network leverages its prior knowledge, which can lead to faster convergence and less requirement for computational resources. It's a balance between utilizing the strength of the pre-trained model and customizing it to fit the specific nuances of the new dataset.\n",
        "\n",
        "Alternatively, we could also **use the architecture without pre-trained weights**, that is, initializing the network with random weights. This usually entails that our dataset is significantly different from the dataset used in the original training (e.g., ImageNet). However, we deem that the architecture itself could be a good fit for the problem, at least for initial investigations."
      ],
      "metadata": {
        "id": "N1pmO5pZMzcL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The walktrough will show you how to use `torchvision.models` to load a `ResNet-18` architecture, with and without pre-trained weights, and how we can enforce the freezing of the feature extraction layers. After the walkthrough, you'll be asked to implement the two transfer learning strategies for our case study on land use identification, compare their effectiveness, also with respect to the models you trained in Part 1 and Part 2 of this case study."
      ],
      "metadata": {
        "id": "xWLA1eiRCyEd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Walkthrough"
      ],
      "metadata": {
        "id": "Xlo6O3jRdUhH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models\n",
        "from torchsummary import summary"
      ],
      "metadata": {
        "id": "DPr9CtVPmp5m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if CUDA is available, otherwise use CPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Using device: {device}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2c5H_MR7mrBv",
        "outputId": "3aa9748f-9c31-4885-edfb-ba7c9fbda630"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using `ResNet-18` without Pre-trained Weights"
      ],
      "metadata": {
        "id": "-mouujKnmnB1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To initialize `ResNet-18` model without pre-trained weights we specify `models.resnet18(weights=None)`. The last layer (the fully connected layer) is responsible for classification. We need to find out the number of features extracted by `ResNet-18`, which is essential for adapting this layer for our speecific task. This is obtained from `resnet18.fc.in_features`. For our specific use case of land use identification, we modify the fully connected layer to predict 21 classes (the number of land use categories in our study). This is done by setting `resnet18.fc` to a new `nn.Linear` layer with `num_ftrs` input features and `num_classes` output features."
      ],
      "metadata": {
        "id": "NCphvxvmDtlN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the ResNet-18 model\n",
        "resnet18 = models.resnet18(weights=None)  # This ensures we do not load the pretrained weights\n",
        "\n",
        "# We need to know the number of features in the last layer of the pretrained model to adapt our classifier.\n",
        "# These are equal to the number of inputs of the \"fully connected\" (fc) head of the ResNet-18.\n",
        "num_ftrs = resnet18.fc.in_features\n",
        "print(f\"The number of features extracted by ResNet-18 are: {num_ftrs}\")\n",
        "\n",
        "# Check the number of output classes of the original ResNet-18\n",
        "num_outputs = resnet18.fc.out_features\n",
        "print(f\"The original ResNet-18 predicts {num_outputs} classes\")\n",
        "\n",
        "# Modify the fully connected layer to match the number of classes for the land use identification case study\n",
        "num_classes = 21\n",
        "resnet18.fc = nn.Linear(num_ftrs, num_classes)\n",
        "num_outputs = resnet18.fc.out_features\n",
        "print(f\"The modified ResNet-18 predicts {num_outputs} classes\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KXNPHHy1j7oR",
        "outputId": "c750fa36-1d50-48dd-b027-0160408bf2cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The number of features extracted by ResNet-18 are: 512\n",
            "The original ResNet-18 predicts 1000 classes\n",
            "The modified ResNet-18 predicts 21 classes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " After the modifications, we move the model to the specified device (e.g., GPU or CPU) and print a summary for a given input size. This summary helps in understanding the model’s architecture and the impact of our modifications.\n",
        " > Note: Historically, architectures pre-trained on ImageNet are trained on 224x224 RGB input images.\n"
      ],
      "metadata": {
        "id": "4uBtmUPVEIcO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the final model and print the summary\n",
        "model = resnet18.to(device)\n",
        "summary(model, input_size=(3, 224, 224))  # Replace with your input size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rwUWcXEAoc_k",
        "outputId": "a55b2dc6-c57c-41b9-ae0f-4ce4e6116291"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1         [-1, 64, 112, 112]           9,408\n",
            "       BatchNorm2d-2         [-1, 64, 112, 112]             128\n",
            "              ReLU-3         [-1, 64, 112, 112]               0\n",
            "         MaxPool2d-4           [-1, 64, 56, 56]               0\n",
            "            Conv2d-5           [-1, 64, 56, 56]          36,864\n",
            "       BatchNorm2d-6           [-1, 64, 56, 56]             128\n",
            "              ReLU-7           [-1, 64, 56, 56]               0\n",
            "            Conv2d-8           [-1, 64, 56, 56]          36,864\n",
            "       BatchNorm2d-9           [-1, 64, 56, 56]             128\n",
            "             ReLU-10           [-1, 64, 56, 56]               0\n",
            "       BasicBlock-11           [-1, 64, 56, 56]               0\n",
            "           Conv2d-12           [-1, 64, 56, 56]          36,864\n",
            "      BatchNorm2d-13           [-1, 64, 56, 56]             128\n",
            "             ReLU-14           [-1, 64, 56, 56]               0\n",
            "           Conv2d-15           [-1, 64, 56, 56]          36,864\n",
            "      BatchNorm2d-16           [-1, 64, 56, 56]             128\n",
            "             ReLU-17           [-1, 64, 56, 56]               0\n",
            "       BasicBlock-18           [-1, 64, 56, 56]               0\n",
            "           Conv2d-19          [-1, 128, 28, 28]          73,728\n",
            "      BatchNorm2d-20          [-1, 128, 28, 28]             256\n",
            "             ReLU-21          [-1, 128, 28, 28]               0\n",
            "           Conv2d-22          [-1, 128, 28, 28]         147,456\n",
            "      BatchNorm2d-23          [-1, 128, 28, 28]             256\n",
            "           Conv2d-24          [-1, 128, 28, 28]           8,192\n",
            "      BatchNorm2d-25          [-1, 128, 28, 28]             256\n",
            "             ReLU-26          [-1, 128, 28, 28]               0\n",
            "       BasicBlock-27          [-1, 128, 28, 28]               0\n",
            "           Conv2d-28          [-1, 128, 28, 28]         147,456\n",
            "      BatchNorm2d-29          [-1, 128, 28, 28]             256\n",
            "             ReLU-30          [-1, 128, 28, 28]               0\n",
            "           Conv2d-31          [-1, 128, 28, 28]         147,456\n",
            "      BatchNorm2d-32          [-1, 128, 28, 28]             256\n",
            "             ReLU-33          [-1, 128, 28, 28]               0\n",
            "       BasicBlock-34          [-1, 128, 28, 28]               0\n",
            "           Conv2d-35          [-1, 256, 14, 14]         294,912\n",
            "      BatchNorm2d-36          [-1, 256, 14, 14]             512\n",
            "             ReLU-37          [-1, 256, 14, 14]               0\n",
            "           Conv2d-38          [-1, 256, 14, 14]         589,824\n",
            "      BatchNorm2d-39          [-1, 256, 14, 14]             512\n",
            "           Conv2d-40          [-1, 256, 14, 14]          32,768\n",
            "      BatchNorm2d-41          [-1, 256, 14, 14]             512\n",
            "             ReLU-42          [-1, 256, 14, 14]               0\n",
            "       BasicBlock-43          [-1, 256, 14, 14]               0\n",
            "           Conv2d-44          [-1, 256, 14, 14]         589,824\n",
            "      BatchNorm2d-45          [-1, 256, 14, 14]             512\n",
            "             ReLU-46          [-1, 256, 14, 14]               0\n",
            "           Conv2d-47          [-1, 256, 14, 14]         589,824\n",
            "      BatchNorm2d-48          [-1, 256, 14, 14]             512\n",
            "             ReLU-49          [-1, 256, 14, 14]               0\n",
            "       BasicBlock-50          [-1, 256, 14, 14]               0\n",
            "           Conv2d-51            [-1, 512, 7, 7]       1,179,648\n",
            "      BatchNorm2d-52            [-1, 512, 7, 7]           1,024\n",
            "             ReLU-53            [-1, 512, 7, 7]               0\n",
            "           Conv2d-54            [-1, 512, 7, 7]       2,359,296\n",
            "      BatchNorm2d-55            [-1, 512, 7, 7]           1,024\n",
            "           Conv2d-56            [-1, 512, 7, 7]         131,072\n",
            "      BatchNorm2d-57            [-1, 512, 7, 7]           1,024\n",
            "             ReLU-58            [-1, 512, 7, 7]               0\n",
            "       BasicBlock-59            [-1, 512, 7, 7]               0\n",
            "           Conv2d-60            [-1, 512, 7, 7]       2,359,296\n",
            "      BatchNorm2d-61            [-1, 512, 7, 7]           1,024\n",
            "             ReLU-62            [-1, 512, 7, 7]               0\n",
            "           Conv2d-63            [-1, 512, 7, 7]       2,359,296\n",
            "      BatchNorm2d-64            [-1, 512, 7, 7]           1,024\n",
            "             ReLU-65            [-1, 512, 7, 7]               0\n",
            "       BasicBlock-66            [-1, 512, 7, 7]               0\n",
            "AdaptiveAvgPool2d-67            [-1, 512, 1, 1]               0\n",
            "           Linear-68                   [-1, 21]          10,773\n",
            "================================================================\n",
            "Total params: 11,187,285\n",
            "Trainable params: 11,187,285\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.57\n",
            "Forward/backward pass size (MB): 62.79\n",
            "Params size (MB): 42.68\n",
            "Estimated Total Size (MB): 106.04\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we iterate through all the parameters of the model and print their `requires_grad` attribute (i.e., requires the computation of the gradient for backpropagation); `requires_grad` = `True` indicates that the training process will affect the specific layer of parameters. We can see here that all layers will be modified, as expected.\n"
      ],
      "metadata": {
        "id": "fHZrMJp9ELdk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Freeze the layers\n",
        "for param in resnet18.parameters():\n",
        "    print(param.requires_grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A06-kIrVk1Kw",
        "outputId": "6707c576-1be8-4266-ef05-e0f8752a697a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fine-tuning the entire `ResNet-18` with Pre-Trained Weights\n"
      ],
      "metadata": {
        "id": "QHf1B8vsFXJM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To utilize the `ResNet-18` model with pre-trained weights, we first import the weights in our Python environment, and then we assign them to an instance of the `ResNet-18` architecture. `ResNet18_Weights.IMAGENET1K_V1` are the weights obtained after training the model on the 1st version of the IMAGENET1K dataset, which contains over a million images across 100 different classes.\n"
      ],
      "metadata": {
        "id": "TcFFfP2jGobk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.models import ResNet18_Weights\n",
        "\n",
        "# Load the ResNet-18 model\n",
        "resnet18 = models.resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)  # This ensures we load the pretrained weights\n",
        "\n",
        "# We need to know the number of features in the last layer of the pretrained model to adapt our classifier.\n",
        "# These are equal to the number of inputs of the \"fully connected\" (fc) head of the ResNet-18.\n",
        "num_ftrs = resnet18.fc.in_features\n",
        "print(f\"The number of features extracted by ResNet-18 are: {num_ftrs}\")\n",
        "\n",
        "# Check the number of output classes of the original ResNet-18\n",
        "num_outputs = resnet18.fc.out_features\n",
        "print(f\"The original ResNet-18 predicts {num_outputs} classes\")\n",
        "\n",
        "# Modify the fully connected layer to match the number of classes for the land use identification case study\n",
        "num_classes = 21\n",
        "resnet18.fc = nn.Linear(num_ftrs, num_classes)\n",
        "num_outputs = resnet18.fc.out_features\n",
        "print(f\"The modified ResNet-18 predicts {num_outputs} classes\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hli_zVB3GY_L",
        "outputId": "98455408-c570-49ba-975e-46250b1a556b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 96.1MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The number of features extracted by ResNet-18 are: 512\n",
            "The original ResNet-18 predicts 1000 classes\n",
            "The modified ResNet-18 predicts 21 classes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The rest is unchanged."
      ],
      "metadata": {
        "id": "8TQGqUqT073k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the final model and print the summary\n",
        "model = resnet18.to(device)\n",
        "summary(model, input_size=(3, 224, 224))  # Replace with your input size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6yJYFPII0ngl",
        "outputId": "b01b979b-1dd8-4acd-934b-ce93bbc0a2cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1         [-1, 64, 112, 112]           9,408\n",
            "       BatchNorm2d-2         [-1, 64, 112, 112]             128\n",
            "              ReLU-3         [-1, 64, 112, 112]               0\n",
            "         MaxPool2d-4           [-1, 64, 56, 56]               0\n",
            "            Conv2d-5           [-1, 64, 56, 56]          36,864\n",
            "       BatchNorm2d-6           [-1, 64, 56, 56]             128\n",
            "              ReLU-7           [-1, 64, 56, 56]               0\n",
            "            Conv2d-8           [-1, 64, 56, 56]          36,864\n",
            "       BatchNorm2d-9           [-1, 64, 56, 56]             128\n",
            "             ReLU-10           [-1, 64, 56, 56]               0\n",
            "       BasicBlock-11           [-1, 64, 56, 56]               0\n",
            "           Conv2d-12           [-1, 64, 56, 56]          36,864\n",
            "      BatchNorm2d-13           [-1, 64, 56, 56]             128\n",
            "             ReLU-14           [-1, 64, 56, 56]               0\n",
            "           Conv2d-15           [-1, 64, 56, 56]          36,864\n",
            "      BatchNorm2d-16           [-1, 64, 56, 56]             128\n",
            "             ReLU-17           [-1, 64, 56, 56]               0\n",
            "       BasicBlock-18           [-1, 64, 56, 56]               0\n",
            "           Conv2d-19          [-1, 128, 28, 28]          73,728\n",
            "      BatchNorm2d-20          [-1, 128, 28, 28]             256\n",
            "             ReLU-21          [-1, 128, 28, 28]               0\n",
            "           Conv2d-22          [-1, 128, 28, 28]         147,456\n",
            "      BatchNorm2d-23          [-1, 128, 28, 28]             256\n",
            "           Conv2d-24          [-1, 128, 28, 28]           8,192\n",
            "      BatchNorm2d-25          [-1, 128, 28, 28]             256\n",
            "             ReLU-26          [-1, 128, 28, 28]               0\n",
            "       BasicBlock-27          [-1, 128, 28, 28]               0\n",
            "           Conv2d-28          [-1, 128, 28, 28]         147,456\n",
            "      BatchNorm2d-29          [-1, 128, 28, 28]             256\n",
            "             ReLU-30          [-1, 128, 28, 28]               0\n",
            "           Conv2d-31          [-1, 128, 28, 28]         147,456\n",
            "      BatchNorm2d-32          [-1, 128, 28, 28]             256\n",
            "             ReLU-33          [-1, 128, 28, 28]               0\n",
            "       BasicBlock-34          [-1, 128, 28, 28]               0\n",
            "           Conv2d-35          [-1, 256, 14, 14]         294,912\n",
            "      BatchNorm2d-36          [-1, 256, 14, 14]             512\n",
            "             ReLU-37          [-1, 256, 14, 14]               0\n",
            "           Conv2d-38          [-1, 256, 14, 14]         589,824\n",
            "      BatchNorm2d-39          [-1, 256, 14, 14]             512\n",
            "           Conv2d-40          [-1, 256, 14, 14]          32,768\n",
            "      BatchNorm2d-41          [-1, 256, 14, 14]             512\n",
            "             ReLU-42          [-1, 256, 14, 14]               0\n",
            "       BasicBlock-43          [-1, 256, 14, 14]               0\n",
            "           Conv2d-44          [-1, 256, 14, 14]         589,824\n",
            "      BatchNorm2d-45          [-1, 256, 14, 14]             512\n",
            "             ReLU-46          [-1, 256, 14, 14]               0\n",
            "           Conv2d-47          [-1, 256, 14, 14]         589,824\n",
            "      BatchNorm2d-48          [-1, 256, 14, 14]             512\n",
            "             ReLU-49          [-1, 256, 14, 14]               0\n",
            "       BasicBlock-50          [-1, 256, 14, 14]               0\n",
            "           Conv2d-51            [-1, 512, 7, 7]       1,179,648\n",
            "      BatchNorm2d-52            [-1, 512, 7, 7]           1,024\n",
            "             ReLU-53            [-1, 512, 7, 7]               0\n",
            "           Conv2d-54            [-1, 512, 7, 7]       2,359,296\n",
            "      BatchNorm2d-55            [-1, 512, 7, 7]           1,024\n",
            "           Conv2d-56            [-1, 512, 7, 7]         131,072\n",
            "      BatchNorm2d-57            [-1, 512, 7, 7]           1,024\n",
            "             ReLU-58            [-1, 512, 7, 7]               0\n",
            "       BasicBlock-59            [-1, 512, 7, 7]               0\n",
            "           Conv2d-60            [-1, 512, 7, 7]       2,359,296\n",
            "      BatchNorm2d-61            [-1, 512, 7, 7]           1,024\n",
            "             ReLU-62            [-1, 512, 7, 7]               0\n",
            "           Conv2d-63            [-1, 512, 7, 7]       2,359,296\n",
            "      BatchNorm2d-64            [-1, 512, 7, 7]           1,024\n",
            "             ReLU-65            [-1, 512, 7, 7]               0\n",
            "       BasicBlock-66            [-1, 512, 7, 7]               0\n",
            "AdaptiveAvgPool2d-67            [-1, 512, 1, 1]               0\n",
            "           Linear-68                   [-1, 21]          10,773\n",
            "================================================================\n",
            "Total params: 11,187,285\n",
            "Trainable params: 11,187,285\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.57\n",
            "Forward/backward pass size (MB): 62.79\n",
            "Params size (MB): 42.68\n",
            "Estimated Total Size (MB): 106.04\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Freeze the layers\n",
        "for param in resnet18.parameters():\n",
        "    print(param.requires_grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f2T_UZPt0ni9",
        "outputId": "ddf88585-ed2b-415c-cca1-f544ecb4eabe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Freezing the feature extraction layers to only fine-tune the classifier"
      ],
      "metadata": {
        "id": "vSeDdfQu0s5p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To freeze all layers of the ResNet model, we need to disable gradient computation for all parameters. This is done by setting `requires_grad` to `False` for all parameters. To ensure that we can still fine-tune the `resnet18.fc` fully connected classifier, we can either add it after freezing the layers, or set `requires_grad=True` afterwards only for this specific layer (as done below)."
      ],
      "metadata": {
        "id": "BbtsL6cW1VZ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the ResNet-18 model\n",
        "resnet18 = models.resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)  # This ensures we load the pretrained weights\n",
        "\n",
        "# We need to know the number of features in the last layer of the pretrained model to adapt our classifier.\n",
        "# These are equal to the number of inputs of the \"fully connected\" (fc) head of the ResNet-18.\n",
        "num_ftrs = resnet18.fc.in_features\n",
        "\n",
        "# Modify the fully connected layer to match the number of classes for the land use identification case study\n",
        "num_classes = 21\n",
        "resnet18.fc = nn.Linear(num_ftrs, num_classes)\n",
        "num_outputs = resnet18.fc.out_features\n",
        "print(f\"The modified ResNet-18 predicts {num_outputs} classes\")\n",
        "\n",
        "# Freeze all the layers\n",
        "for param in resnet18.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Unfreeze classifier\n",
        "for param in resnet18.fc.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "# Check that only the last two layers are not frozen (hidden and output layer of the classifier)\n",
        "for param in resnet18.parameters():\n",
        "  print(param.requires_grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W5V_jWKX0p-6",
        "outputId": "8742336b-cb52-4c1f-80ff-d96628030aee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The modified ResNet-18 predicts 21 classes\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "True\n",
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Normalization to use Pre-trained Model"
      ],
      "metadata": {
        "id": "A9v8tpoQ3qTT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When fine-tuning the pre-trained architecture on your dataset, it is crucial to apply the same data normalization that was used during the initial training of the network. For models trained on the ImageNet dataset, which includes the standard `ResNet-18` used here, the common practice is to normalize the input data using the *mean* and *standard deviation* of the ImageNet dataset. These values are:\n",
        "\n",
        "* Mean: [0.485, 0.456, 0.406] for the RGB channels, reespectively;\n",
        "* Standard Deviation: [0.229, 0.224, 0.225] for the RGB channels, reespectively.\n"
      ],
      "metadata": {
        "id": "eg1ClXCI3vZU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can implement this normalization in `PyTorch` as part of the data preprocessing pipeline, using `transforms`:\n",
        "\n",
        "```\n",
        "# Define the transformation\n",
        "transform = transforms.Compose([\n",
        "    ...\n",
        "    ...\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "```\n",
        "\n",
        "Also, remember to make sure you input images are 224x224 pixels RGB images."
      ],
      "metadata": {
        "id": "iH_aE5RA4WAn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Assignments"
      ],
      "metadata": {
        "id": "G738PDfh5PZw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The instructions below describe the assignments for this notebook. Carrying them out will improve your understanding of the benefits of transfer learning and how to implement it practically. Work with your fellow classmates to speed-up implementation and facilitate discussion. Start from the 1st exercise and proceed sequentially.\n",
        "\n",
        "You are tasked to:\n",
        "\n",
        "1. **Implement the Training/Validation/Testing pipeline**:\n",
        "  - You should be able to copy/paste and adapt what you have done in the previous notebooks, including data loading a dataset creation.\n",
        "  - The \"training\" step is now carried out by fine-tuning a pre-trained `ResNet18`\n",
        "  - Make sure your pipeline includes normalization/resizing to account for ImageNet pre-training (e.g., mean/std and 224x224 input images)\n",
        "\n",
        "2. **Fine-Tuning ResNet-18 in both modalities**:\n",
        "  - Fine-tuning all layers.\n",
        "  - Fine-tuning the classifier (fully connected) layer alone.\n",
        "  - Analyze and compare the accuracy, loss, and convergence time of each model.\n",
        "  - Discuss the benefits and drawbacks of each strategy.\n",
        "\n",
        "3. **Comparison with Basic CNN**:\n",
        "  - Use the performance metrics of the basic CNN model you have developed previously as a baseline.\n",
        "  - Compare this baseline with the performances of the fine-tuned ResNet-18 models.\n",
        "  - Analyze which model performs better and under what circumstances.\n",
        "\n",
        "4. **Investigating Hyper-parameters**:\n",
        "  - Experiment with different learning rates and fine-tuning dataset sizes.\n",
        "  - Observe and record how these changes affect the model's performance and training efficiency.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "z4B-9l8b5RO-"
      }
    }
  ]
}